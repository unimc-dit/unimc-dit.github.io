<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>UniMC Project Page</title>
<style>
/* CSS Document */
@import url(https://fonts.googleapis.com/css?family=Open+Sans);
@import url(https://fonts.googleapis.com/css?family=Google+Sans);

body {
  font-family: 'Open-Sans', sans-serif;
  font-weight: 300;
  background-color: #f0f0f0; /* Changed background for better contrast */
}
.content {
  width: 1000px;
  padding: 25px 50px;
  margin: 25px auto;
  background-color: white;
  box-shadow: 0px 0px 10px #999;
  border-radius: 15px;
  font-family: "Google Sans", sans-serif; /* Added fallback font */
}
a, a:visited {
  color: #224b8d;
  font-weight: 300;
  text-decoration: none; /* Removed underline for a cleaner look */
}
a:hover {
    text-decoration: underline;
}
#authors {
  text-align: center;
  margin-bottom: 20px;
  font-size: 20px;
}
#authors a {
  margin: 0 10px;
}
h1 {
  text-align: center;
  font-size: 35px;
  font-weight: 400; /* Made it a bit bolder */
  color: #333;
}
h2 {
  font-size: 30px;
  font-weight: 300;
  border-bottom: 1px solid #eee; /* Added a subtle separator */
  padding-bottom: 10px;
  margin-top: 40px;
}
h3 {
    font-weight: 300;
}
code {
  display: block;
  padding: 15px;
  margin: 10px 10px;
  background-color: #f8f8f8;
  border: 1px solid #ddd;
  border-radius: 5px;
  font-family: "Courier New", Courier, monospace;
  white-space: pre-wrap; /* Ensures long lines wrap */
  line-height: 1.6; /* Improved line spacing for readability */
}
p {
  line-height: 28px; /* Increased line height for readability */
  text-align: justify;
  font-size: 16px;
}
p code {
  display: inline;
  padding: 2px 4px;
  margin: 0 2px;
  background-color: #f8f8f8;
  border-radius: 3px;
  font-size: 0.9em;
}
.teaser-gif {
  display: block;
  margin-left: auto;
  margin-right: auto;
  border-radius: 10px; /* Added rounded corners to images */
  box-shadow: 0px 0px 8px #aaa;
}
.summary-img {
  width: 100%;
  display: block;
  margin-left: auto;
  margin-right: auto;
  border-radius: 10px;
  margin-top: 15px;
  margin-bottom: 15px;
}
</style>
</head>

<body>
<div class="content">
  <h1><strong>UniMC: Taming Diffusion Transformer for Unified Keypoint-Guided Multi-Class Image Generation</strong></h1>
  <p id="authors">
    <span>Qin Guo<sup>1</sup>, Ailing Zeng<sup>2</sup>, Dongxu Yue<sup>3</sup>, Ceyuan Yang<sup>4</sup>, Yang Cao<sup>1</sup>, Hanzhong Guo<sup>5</sup>, Fei Shen<sup>6</sup>, Wei Liu<sup>2</sup>, Xihui Liu<sup>5</sup>, Dan Xu<sup>1</sup></span>
    <br><br>
    <span style="font-size: 18px; color: #555;">
        <sup>1</sup>The Hong Kong University of Science and Technology, <sup>2</sup>Tencent, <sup>3</sup>Peking University, <sup>4</sup>The Chinese University of Hong Kong, <sup>5</sup>The University of Hong Kong, <sup>6</sup>National University of Singapore
    </span>
  </p>
  <br>
  <img src="./figs/teaser.png" alt="UniMC Teaser Image" class="teaser-gif" style="width:100%;">
  <h3 style="text-align:center"><em><b>Top:</b> We establish HAIG-2.9M, a large-scale, high-quality, and highly diverse dataset with joint keypoint-level, instance-level, and densely semantic annotations for both humans and animals. <b>Bottom:</b> Based on HAIG-2.9M, we design UniMC, a controllable DiT-based framework for keypoint-guided image generation, especially for multi-class and heavy occlusion scenarios. The bottom part of the figure showcases samples generated by UniMC.</em></h3>
    <font size="+2">
          <p style="text-align: center; margin-top: 20px; margin-bottom: 20px;">
            <a href="https://arxiv.org/abs/2507.02713" target="_blank">[Paper]</a> &nbsp;&nbsp;&nbsp;&nbsp;
            <a href="https://unimc-dit.github.io" target="_blank">[Project Page]</a> &nbsp;&nbsp;&nbsp;&nbsp;
          </p>
    </font>
</div>
<div class="content">
  <h2 style="text-align:center;">Abstract</h2>
  <p>Although significant advancements have been achieved in the progress of keypoint-guided Text-to-Image diffusion models, existing mainstream keypoint-guided models encounter challenges in controlling the generation of more general non-rigid objects beyond humans (<i>e.g.</i>, animals). Moreover, it is difficult to generate multiple overlapping humans and animals based on keypoint controls solely. These challenges arise from two main aspects: the inherent limitations of existing controllable methods and the lack of suitable datasets. First, we design a DiT-based framework, named UniMC, to explore unifying controllable multi-class image generation. UniMC integrates instance- and keypoint-level conditions into compact tokens, incorporating attributes such as class, bounding box, and keypoint coordinates. This approach overcomes the limitations of previous methods that struggled to distinguish instances and classes due to their reliance on skeleton images as conditions. Second, we propose HAIG-2.9M, a large-scale, high-quality, and diverse dataset designed for keypoint-guided human and animal image generation. HAIG-2.9M includes 786K images with 2.9M instances. This dataset features extensive annotations such as keypoints, bounding boxes, and fine-grained captions for both humans and animals, along with rigorous manual inspection to ensure annotation accuracy. Extensive experiments demonstrate the high quality of HAIG-2.9M and the effectiveness of UniMC, particularly in heavy occlusions and multi-class scenarios.</p>
</div>
<div class="content">
  <h2>Background</h2>
  <p>The generation of non-rigid objects, such as humans and animals, is vital across various domains. While previous works on keypoint-guided human image generation have utilized skeleton images as structural conditions, it is difficult for those methods to generate multi-class, multi-instance images of humans and/or animals, especially in scenarios with overlapping instances. This is due to limitations in both the conditional formulation (class and instance binding confusion) and the available datasets. Most methods plot keypoints on images as control signals, which leads to ambiguity. For example, it's hard to distinguish if keypoints should depict a cat or a dog, and challenging to determine which keypoint belongs to which instance in overlapping regions. Furthermore, existing datasets are often limited to humans or animals separately and lack the quality and scale needed for modern generative models.</p>
  <br>
  <img class="summary-img" src="./figs/condition_form.png" alt="Condition Form Limitations">
  <p style="text-align:center; margin-top: 10px; font-style: italic; color: #666;">Skeleton image conditions face two main issues: (a) Class binding confusion and (b) Instance binding confusion.</p>
</div>
<div class="content">
  <h2>The HAIG-2.9M Dataset</h2>
  <p>To overcome the limitations of existing datasets, we introduce HAIG-2.9M. It includes 786K images with an average aesthetic score of 5.91, covering 31 species classes, and annotated with 2.9M instance-level bounding boxes, keypoints, and captions, averaging 3.66 instances per image. To ensure data quality and diversity, we sourced images from high-quality non-commercial websites and filtered large-scale datasets. We then used state-of-the-art models for annotation, with rigorous manual checks to ensure accuracy.</p>
  <br>
  <img class="summary-img" src="./figs/data_collection.png" alt="Data Collection Pipeline">
  <p style="text-align:center; margin-top: 10px; font-style: italic; color: #666;">Our dataset collection and annotation pipeline ensures high quality and diversity.</p>
</div>
<div class="content">
  <h2>Approach: The UniMC Framework</h2>
  <p>To achieve unified human and animal generation, we present UniMC, a Diffusion Transformer (DiT)-based framework. Instead of skeleton images, we use compact and informative conditions: the keypoint coordinates, bounding box coordinates, and class names for each instance. This allows for class- and instance-aware unified encoding, avoiding the confusion issues of previous methods.</p>
  <br>
  <img class="summary-img" src="./figs/kpt_attention.png" alt="UniMC Framework Architecture">
  <p>Our framework consists of two key components:</p>
  <p><b>1. Unified Keypoint Encoder:</b> A lightweight module that encodes keypoints and attributes (class, bounding box) from different species into a unified representation space. This addresses the semantic and structural differences across classes and avoids the need for separate encoders. Invisible or missing parts are handled using a learnable mask token.</p>
  <p><b>2. Timestep-aware Keypoint Modulator:</b> We insert self-attention layers into the blocks of the DiT backbone (PixArt-&alpha;) to model the relationship between the unified keypoint tokens and the backbone feature tokens. This modulator is designed to be aware of both the image structure and the diffusion timestep, enabling effective keypoint-level control throughout the generation process.</p>
</div>
<div class="content">
  <h2>Results</h2>
  <p>We compare UniMC with representative methods like the base PixArt-&alpha; model, ControlNet (using skeleton images), and GLIGEN (using keypoint coordinates). Our qualitative and quantitative results demonstrate the superiority of UniMC. It achieves fine-grained control of poses for different classes and instances while maintaining high-quality generation, even in challenging scenarios with heavy occlusion. The results also highlight the importance of our HAIG-2.9M dataset; models trained on it significantly outperform those trained on combined existing datasets like COCO and APT36K.</p>
  <img class="summary-img" src="./figs/qualitative.png" alt="Qualitative Comparison of Results">
  <p style="text-align:center; margin-top: 10px; font-style: italic; color: #666;"><b>Qualitative Comparisons:</b> UniMC (b) successfully generates complex, multi-class scenes with accurate poses, outperforming ControlNet (f) and GLIGEN (g), and showing the benefit of the HAIG-2.9M dataset over COCO (c), APT36K (d), or their combination (e).</p>
</div>
<div class="content">
  <h2>Impact Statement</h2>
  <p>This paper presents work whose goal is to advance the field of Machine Learning. Our model enhances creativity with precise keypoint-level control but carries misuse risks. We emphasize responsible use and transparency.</p>
</div>
<div class="content">
  <h2>BibTeX</h2>
  <code>@inproceedings{guo2025unimc,
  title={Uni{MC}: Taming Diffusion Transformer for Unified Keypoint-Guided Multi-Class Image Generation},
  author={Qin Guo and Ailing Zeng and Dongxu Yue and Ceyuan Yang and Yang Cao and Hanzhong Guo and Fei Shen and Wei Liu and Xihui Liu and Dan Xu},
  booktitle={Forty-second International Conference on Machine Learning},
  year={2025}
}</code>
</div>
</body>
</html>
